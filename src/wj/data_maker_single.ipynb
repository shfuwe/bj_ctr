{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def process_date_time(this_df):\n",
    "    if 'pubtime' in this_df.columns:\n",
    "        this_df['dt'] = pd.to_datetime(this_df['pubtime'], utc=True, unit='ms').dt.tz_convert('Asia/Shanghai')\n",
    "    if 'timestamp' in this_df.columns:\n",
    "        this_df['dt'] = pd.to_datetime(this_df['timestamp'], utc=True, unit='ms').dt.tz_convert('Asia/Shanghai')\n",
    "    if 'pubtime' in this_df.columns:\n",
    "        print('process test data date')\n",
    "        this_df['pub_date'] = this_df['dt'].dt.date\n",
    "        this_df['pub_date'] = this_df['pub_date'].astype('str')\n",
    "        lbe = LabelEncoder()\n",
    "        this_df['pub_date'] = lbe.fit_transform(this_df['pub_date'])\n",
    "        del this_df['pubtime']\n",
    "        del this_df['dt']\n",
    "    if 'timestamp' in this_df.columns:\n",
    "        print('process train data date')\n",
    "#         this_df['inter_day'] = this_df['dt'].dt.day\n",
    "#         this_df['inter_day'].fillna(0, inplace=True)\n",
    "#         this_df[['inter_day']] = this_df[['inter_day']].astype(int)\n",
    "#         lbe = LabelEncoder()\n",
    "#         this_df['inter_day'] = lbe.fit_transform(this_df['inter_day'])\n",
    "#         this_df['inter_hour'] = round(this_df['dt'].dt.hour)\n",
    "#         this_df['inter_hour'].fillna(0, inplace=True)\n",
    "#         this_df[['inter_hour']] = this_df[['inter_hour']].astype(int)\n",
    "#         lbe = LabelEncoder()\n",
    "#         this_df['inter_hour'] = lbe.fit_transform(this_df['inter_hour'])\n",
    "        this_df['hour'] = round(this_df['dt'].dt.hour/2)\n",
    "        hour_mean=round(this_df['hour'].mean())\n",
    "        print('hour_mean:',hour_mean)\n",
    "        this_df['hour'].fillna(hour_mean, inplace=True)\n",
    "    \n",
    "        del this_df['timestamp']\n",
    "        del this_df['dt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin process news data\n",
      "process test data date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create word book......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347466/347466 [00:12<00:00, 26811.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate new key words representation......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 347466/347466 [00:00<00:00, 861507.14it/s]\n",
      "100%|██████████| 347466/347466 [00:00<00:00, 776947.56it/s]\n",
      "100%|██████████| 347466/347466 [00:00<00:00, 699630.23it/s]\n",
      "100%|██████████| 347466/347466 [00:00<00:00, 727863.26it/s]\n",
      "100%|██████████| 347466/347466 [00:00<00:00, 746477.58it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_info = pd.read_table('../../data/news_info_new_id.txt',header=None)\n",
    "doc_info.columns = ['docid', 'title', 'pubtime', 'picnum', 'category1st', 'category2nd', 'keyword']\n",
    "'''\n",
    "预处理文档信息数据\n",
    ":param filedir:\n",
    ":return:\n",
    "'''\n",
    "print('begin process news data')\n",
    "\n",
    "\n",
    "def clean_picnum(x):\n",
    "#     print(x)\n",
    "    # 对picnum数据进行清洗\n",
    "    if x in ['上海', '云南', '山东', 'NoneType', ''] or x is None or math.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return int(float(x))\n",
    "    \n",
    "doc_info['picnum'] = doc_info['picnum'].apply(lambda x: clean_picnum(x))\n",
    "\n",
    "process_date_time(doc_info)\n",
    "\n",
    "for col in tqdm(['category1st', 'category2nd']):\n",
    "    lbe = LabelEncoder()\n",
    "    doc_info[col] = doc_info[col].fillna('NAN')\n",
    "    doc_info[col] = lbe.fit_transform(doc_info[col])\n",
    "\n",
    "if 'title' in doc_info.columns:\n",
    "    del doc_info['title']\n",
    "\n",
    "doc_info['docid'] = doc_info['docid'].astype('int')\n",
    "\n",
    "print('create word book......')\n",
    "words = [[], [], [], [], []]\n",
    "\n",
    "for i in tqdm(doc_info['keyword']):\n",
    "    key_one = {}\n",
    "    try:\n",
    "        splts = i.split(',')\n",
    "        for sp in splts:\n",
    "            split_ = sp.split(':')\n",
    "            try:\n",
    "                score = float(split_[1])\n",
    "                key_one[split_[0]] = score\n",
    "            except:\n",
    "                continue\n",
    "        key_one = sorted(key_one.items(), key=lambda x: x[1], reverse=True)\n",
    "    except:\n",
    "        key_one = {}\n",
    "    for j in range(5):\n",
    "        try:\n",
    "            key_ = key_one[j][0]\n",
    "            words[j].append(key_)\n",
    "        except:\n",
    "            words[j].append('_#_#_#_')\n",
    "        \n",
    "keywords_all = ['_#_#_#_']\n",
    "for i in range(5):\n",
    "    keywords_all += words[i]\n",
    "\n",
    "keywords_all = list(set(keywords_all))\n",
    "keyword2id = dict(zip(keywords_all, range(1, len(keywords_all) + 1)))\n",
    "\n",
    "print('generate new key words representation......')\n",
    "for i in range(5):\n",
    "    for j in tqdm(range(len(words[i]))):\n",
    "        words[i][j] = keyword2id[words[i][j]]\n",
    "\n",
    "doc_info['keyword0'] = words[0]\n",
    "doc_info['keyword1'] = words[1]\n",
    "doc_info['keyword2'] = words[2]\n",
    "doc_info['keyword3'] = words[3]\n",
    "doc_info['keyword4'] = words[4]\n",
    "\n",
    "doc_info_key_word_list = list(doc_info['keyword0']) + list(doc_info['keyword1']) + list(doc_info['keyword2']) \\\n",
    "                   + list(doc_info['keyword3']) + list(doc_info['keyword4'])\n",
    "doc_keyword_id = list(set(doc_info_key_word_list))\n",
    "new_doc_keyword_id = range(1, len(doc_keyword_id) + 1)\n",
    "doc_id_dict = dict(zip(doc_keyword_id, new_doc_keyword_id))\n",
    "# print(len(doc_keyword_id))\n",
    "# 重新设置文档的关键词id\n",
    "doc_info['keyword0'] = doc_info['keyword0'].map(doc_id_dict)\n",
    "doc_info['keyword1'] = doc_info['keyword1'].map(doc_id_dict)\n",
    "doc_info['keyword2'] = doc_info['keyword2'].map(doc_id_dict)\n",
    "doc_info['keyword3'] = doc_info['keyword3'].map(doc_id_dict)\n",
    "doc_info['keyword4'] = doc_info['keyword4'].map(doc_id_dict)\n",
    "\n",
    "if 'keyword' in doc_info.columns:\n",
    "    del doc_info['keyword']\n",
    "doc_info.to_pickle('../../data/wj/doc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed user info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_info = pd.read_table('../../data/user_info_new_id.txt',header=None)\n",
    "user_info.columns = ['userid', 'device', 'os', 'province', 'city', 'age', 'gender']\n",
    "'''\n",
    "    预处理用户数据\n",
    "    :param filedir:\n",
    "    :return:\n",
    "'''\n",
    "def get_cate(x):\n",
    "    if type(x) == float:\n",
    "        return x\n",
    "    li = x.split(',')\n",
    "    res = list()\n",
    "    for i in li:\n",
    "        lbl, prob = i.split(':')\n",
    "        res.append([lbl, float(prob)])\n",
    "    res = sorted(res, key=lambda x: x[1])\n",
    "    return res[-1][0]\n",
    "\n",
    "user_info['age'] = user_info['age'].apply(lambda x: get_cate(x))\n",
    "user_info['gender'] = user_info['gender'].apply(lambda x: get_cate(x))\n",
    "\n",
    "# label encoding 数字化\n",
    "for col in tqdm(\n",
    "        ['device', 'os', 'province', 'city', 'age', 'gender']):\n",
    "    lbe = LabelEncoder()\n",
    "    user_info[col] = user_info[col].fillna('NAN')\n",
    "    user_info[col] = lbe.fit_transform(user_info[col])\n",
    "\n",
    "print('processed user info')\n",
    "user_info.to_pickle('../../data/wj/user.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin process test data\n",
      "process train data date\n",
      "hour_mean: 7\n",
      "done!the total test number is 50000\n"
     ]
    }
   ],
   "source": [
    "test_info = pd.read_table('../../data/test_info_new_id.txt',header=None)\n",
    "test_info.columns = ['id', 'userid', 'docid', 'timestamp', 'network', 'refresh']\n",
    "'''\n",
    "处理测试集数据\n",
    ":param filename:\n",
    ":return:\n",
    "'''\n",
    "\n",
    "print('begin process test data')\n",
    "process_date_time(test_info)\n",
    "print('done!the total test number is {number}'.format(number=test_info.shape[0]))\n",
    "test_info.to_pickle('../../data/wj/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = pd.read_table('../../data/train_info_new_id.txt',header=None)\n",
    "train_info.columns = ['userid', 'docid', 'timestamp', 'network', 'refresh', 'position', 'click', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin process train data\n",
      "process train data date\n",
      "hour_mean: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "处理训练集数据\n",
    ":param filename:\n",
    ":return:\n",
    "'''\n",
    "print('begin process train data')\n",
    "\n",
    "train_info['timestamp'].fillna(1624680000000, inplace=True)\n",
    "\n",
    "process_date_time(train_info)\n",
    "train_info.to_pickle('../../data/wj/train.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bj2",
   "language": "python",
   "name": "bj2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
