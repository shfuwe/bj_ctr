{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import time, json, datetime\n",
    "from tqdm import tqdm\n",
    "import sys, getopt\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8,\n",
    "                 hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]):\n",
    "        \"\"\"\n",
    "        cate_fea_nuniqs: 类别特征的唯一值个数列表，也就是每个类别特征的vocab_size所组成的列表\n",
    "        nume_fea_size: 数值特征的个数，该模型会考虑到输入全为类别型，即没有数值特征的情况\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cate_fea_size = len(cate_fea_nuniqs)\n",
    "        self.nume_fea_size = nume_fea_size\n",
    "\n",
    "        \"\"\"FM部分\"\"\"\n",
    "        # 一阶\n",
    "        if self.nume_fea_size != 0:\n",
    "            self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1)  # 数值特征的一阶表示\n",
    "        self.fm_1st_order_sparse_emb = nn.ModuleList([\n",
    "            nn.Embedding(voc_size, 1) for voc_size in cate_fea_nuniqs])  # 类别特征的一阶表示\n",
    "        self.fm_1st_order_title = nn.Linear(128, 1)\n",
    "        \n",
    "        # 二阶\n",
    "        self.fm_2nd_order_sparse_emb = nn.ModuleList([\n",
    "            nn.Embedding(voc_size, emb_size) for voc_size in cate_fea_nuniqs])  # 类别特征的二阶表示\n",
    "\n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        self.all_dims = [self.cate_fea_size * emb_size] + hid_dims\n",
    "        self.dense_linear = nn.Linear(self.nume_fea_size, self.cate_fea_size * emb_size)  # 数值特征的维度变换到FM输出维度一致\n",
    "        self.title_linear = nn.Linear(128, self.cate_fea_size * emb_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # for DNN\n",
    "        for i in range(1, len(self.all_dims)):\n",
    "            setattr(self, 'linear_' + str(i), nn.Linear(self.all_dims[i - 1], self.all_dims[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(self.all_dims[i]))\n",
    "            setattr(self, 'activation_' + str(i), nn.ReLU())\n",
    "            setattr(self, 'dropout_' + str(i), nn.Dropout(dropout[i - 1]))\n",
    "        # for output\n",
    "        self.dnn_linear = nn.Linear(hid_dims[-1], num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X_sparse, X_dense, X_title):\n",
    "        \"\"\"\n",
    "        X_sparse: 类别型特征输入  [bs, cate_fea_size]\n",
    "        X_dense: 数值型特征输入（可能没有）  [bs, dense_fea_size]\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"FM 一阶部分\"\"\"\n",
    "        fm_1st_sparse_res = [emb(X_sparse[:, i].unsqueeze(1)).view(-1, 1)\n",
    "                             for i, emb in enumerate(self.fm_1st_order_sparse_emb)]\n",
    "        fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim=1)  # [bs, cate_fea_size]\n",
    "        fm_1st_sparse_res = torch.sum(fm_1st_sparse_res, 1, keepdim=True)  # [bs, 1]\n",
    "        \n",
    "        fm_1st_title_res=self.fm_1st_order_title(X_title)\n",
    "        \n",
    "        if X_dense is not None:\n",
    "            fm_1st_dense_res = self.fm_1st_order_dense(X_dense)\n",
    "            fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res+fm_1st_title_res\n",
    "        else:\n",
    "            fm_1st_part = fm_1st_sparse_res+fm_1st_title_res  # [bs, 1]\n",
    "        \"\"\"FM 一阶部分\"\"\"\n",
    "        \n",
    "        \"\"\"FM 二阶部分\"\"\"\n",
    "        fm_2nd_order_res = [emb(X_sparse[:, i].unsqueeze(1)) for i, emb in enumerate(self.fm_2nd_order_sparse_emb)]\n",
    "        fm_2nd_concat_1d = torch.cat(fm_2nd_order_res, dim=1)  # [bs, n, emb_size]  n为类别型特征个数(cate_fea_size)\n",
    "\n",
    "        # 先求和再平方\n",
    "        sum_embed = torch.sum(fm_2nd_concat_1d, 1)  # [bs, emb_size]\n",
    "        square_sum_embed = sum_embed * sum_embed  # [bs, emb_size]\n",
    "        # 先平方再求和\n",
    "        square_embed = fm_2nd_concat_1d * fm_2nd_concat_1d  # [bs, n, emb_size]\n",
    "        sum_square_embed = torch.sum(square_embed, 1)  # [bs, emb_size]\n",
    "        # 相减除以2\n",
    "        sub = square_sum_embed - sum_square_embed\n",
    "        sub = sub * 0.5  # [bs, emb_size]\n",
    "\n",
    "        fm_2nd_part = torch.sum(sub, 1, keepdim=True)  # [bs, 1]\n",
    "        \"\"\"FM 二阶部分\"\"\"\n",
    "        \n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        dnn_out = torch.flatten(fm_2nd_concat_1d, 1)  # [bs, n * emb_size]\n",
    "        if X_title is not None:\n",
    "            title_out=self.relu(self.title_linear(X_title))\n",
    "            dnn_out= dnn_out+title_out\n",
    "        \n",
    "        if X_dense is not None:\n",
    "            dense_out = self.relu(self.dense_linear(X_dense))  # [bs, n * emb_size]\n",
    "            dnn_out = dnn_out + dense_out  # [bs, n * emb_size]\n",
    "\n",
    "        for i in range(1, len(self.all_dims)):\n",
    "            dnn_out = getattr(self, 'linear_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'batchNorm_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'activation_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'dropout_' + str(i))(dnn_out)\n",
    "\n",
    "        dnn_out = self.dnn_linear(dnn_out)  # [bs, 1]\n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        \n",
    "        out = fm_1st_part + fm_2nd_part + dnn_out  # [bs, 1]\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, valid_loader, epochs, optimizer, loss_fcn, scheduler, device,best_auc,j,ii):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    import time\n",
    "    for _ in range(epochs):\n",
    "        \"\"\"训练部分\"\"\"\n",
    "        model.train()\n",
    "        print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        write_log('Epoch: {}:{}'.format(j,ii))\n",
    "        train_loss_sum = 0.0\n",
    "        start_time = time.time()\n",
    "        for idx, x in enumerate(train_loader):\n",
    "            cate_fea, nume_fea,title_fea, label = x[0], x[1], x[2] ,x[3]\n",
    "            cate_fea, nume_fea,title_fea, label = cate_fea.to(device), nume_fea.to(device),title_fea.to(device), label.float().to(device)\n",
    "            pred = model(cate_fea, nume_fea,title_fea).view(-1)\n",
    "            loss = loss_fcn(pred, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum = train_loss_sum+loss.cpu().item()\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == len(train_loader):\n",
    "                write_log(\"Epoch {:04d} | Step {:04d} / {} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                    _ + 1, idx + 1, len(train_loader), train_loss_sum / (idx + 1), time.time() - start_time))\n",
    "        scheduler.step()\n",
    "        \"\"\"推断部分\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_labels, valid_preds = [], []\n",
    "            for idx, x in tqdm(enumerate(valid_loader)):\n",
    "                cate_fea, nume_fea, title_fea, label = x[0], x[1], x[2] ,x[3]\n",
    "                cate_fea, nume_fea, title_fea = cate_fea.to(device), nume_fea.to(device),title_fea.to(device)\n",
    "                pred = model(cate_fea, nume_fea,title_fea).reshape(-1).data.cpu().numpy().tolist()\n",
    "                valid_preds.extend(pred)\n",
    "                valid_labels.extend(label.cpu().numpy().tolist())\n",
    "        cur_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "        if cur_auc > best_auc:\n",
    "            best_auc = cur_auc\n",
    "            import time\n",
    "            end=time.time()\n",
    "            torch.save(model.state_dict(), \"../../data/wj/deepfm_best/10 4.0/deepfm_best_10_title_\"+str(round(best_auc,4))+\"_\"+str(j)+\"_\"+str(ii)+\"_\"+str(time.strftime('%m_%d_%H_%M_%S'))+\".pth\")\n",
    "        write_log('Current AUC: %.6f, Best AUC: %.6f\\n' % (cur_auc, best_auc))\n",
    "    return best_auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "def write_log(w):\n",
    "    file_name = '../../data/wj/' + datetime.date.today().strftime('%m%d') + \"_{}.log\".format(\"deepfm\")\n",
    "    t0 = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(info + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_feat = pd.read_pickle('../../data/wj/doc.pkl')\n",
    "user_feat = pd.read_pickle('../../data/wj/user.pkl')\n",
    "df_test_user_doc=pd.read_pickle('../../data/wj/df_test_user_doc_64.pkl')\n",
    "\n",
    "sparse_features = ['userid', 'docid', 'network', 'hour', 'device', 'os', 'province',\n",
    "                   'city', 'age', 'gender', 'category1st', 'category2nd',\n",
    "                   'pub_date', 'keyword0', 'keyword1', 'keyword2', 'keyword3', 'keyword4']\n",
    "\n",
    "dense_features = ['refresh', 'picnum',\n",
    "                  'userid_click_mean','userid_click_count' ,'userid_duration_mean' ,'userid_picnum_mean',\n",
    "                    'docid_click_mean','docid_click_count','docid_duration_mean','docid_picnum_mean',\n",
    "                    'category1st_click_mean','category1st_click_count','category1st_duration_mean','category1st_picnum_mean',\n",
    "                    'category2nd_click_mean','category2nd_click_count','category2nd_duration_mean','category2nd_picnum_mean',\n",
    "                    'keyword0_click_mean','keyword0_click_count','keyword0_duration_mean','keyword0_picnum_mean',\n",
    "                 'network_click_mean', 'network_click_count', 'network_duration_mean', \n",
    "                  'refresh_click_mean', 'refresh_click_count', 'refresh_duration_mean',\n",
    "                  'device_click_mean', 'device_click_count', 'device_duration_mean', \n",
    "                  'os_click_mean', 'os_click_count', 'os_duration_mean', \n",
    "                  'province_click_mean', 'province_click_count', 'province_duration_mean', \n",
    "                  'city_click_mean', 'city_click_count', 'city_duration_mean', \n",
    "                  'age_click_mean', 'age_click_count', 'age_duration_mean', \n",
    "                  'gender_click_mean', 'gender_click_count', 'gender_duration_mean'\n",
    "                 ]\n",
    "\n",
    "cate_fea_nuniqs = []\n",
    "cate_fea_nuniqs.append(user_feat['userid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(doc_feat['docid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(6)  # network\n",
    "cate_fea_nuniqs.append(13)  # hour\n",
    "cate_fea_nuniqs.append(user_feat['device'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['os'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['province'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['city'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['age'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['gender'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category1st'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category2nd'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['pub_date'].nunique())\n",
    "keyword_nunique = max(doc_feat['keyword0'].max(), doc_feat['keyword1'].max(), doc_feat['keyword2'].max()\n",
    "                      , doc_feat['keyword3'].max(), doc_feat['keyword4'].max()) + 1\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total': 23947711, 'Trainable': 23947711}\n"
     ]
    }
   ],
   "source": [
    "model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))\n",
    "device = torch.device('cuda:4') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "loss_fcn = nn.BCELoss()\n",
    "loss_fcn = loss_fcn.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "# 打印模型参数\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n",
    "\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "title_=pd.read_pickle('../../data/wj/title_embedding.pkl')\n",
    "del title_['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8167113\n",
      "train\n",
      "Current lr : 0.005\n",
      "09:20:15 : Epoch: 0:0\n",
      "09:20:35 : Epoch 0001 | Step 0050 / 748 | Loss 13.3075 | Time 19.5002\n",
      "09:20:54 : Epoch 0001 | Step 0100 / 748 | Loss 9.2765 | Time 38.6155\n",
      "09:21:13 : Epoch 0001 | Step 0150 / 748 | Loss 6.9627 | Time 57.6423\n",
      "09:21:32 : Epoch 0001 | Step 0200 / 748 | Loss 5.4761 | Time 76.9202\n",
      "09:21:51 : Epoch 0001 | Step 0250 / 748 | Loss 4.5248 | Time 95.8876\n",
      "09:22:10 : Epoch 0001 | Step 0300 / 748 | Loss 3.8671 | Time 114.0400\n",
      "09:22:27 : Epoch 0001 | Step 0350 / 748 | Loss 3.3871 | Time 131.9734\n",
      "09:22:45 : Epoch 0001 | Step 0400 / 748 | Loss 3.0202 | Time 149.8616\n",
      "09:23:03 : Epoch 0001 | Step 0450 / 748 | Loss 2.7317 | Time 167.3920\n",
      "09:23:21 : Epoch 0001 | Step 0500 / 748 | Loss 2.4988 | Time 185.3248\n",
      "09:23:38 : Epoch 0001 | Step 0550 / 748 | Loss 2.3071 | Time 202.5373\n",
      "09:23:56 : Epoch 0001 | Step 0600 / 748 | Loss 2.1466 | Time 220.8072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5293/3428872037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mepoch_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mbest_auc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_auc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5293/721814111.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_loader, valid_loader, epochs, optimizer, loss_fcn, scheduler, device, best_auc, j, ii)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mcate_fea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnume_fea\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle_fea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mcate_fea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnume_fea\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle_fea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcate_fea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnume_fea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle_fea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/sw0/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/sw0/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/sw0/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/sw0/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "len_=81671133\n",
    "best_auc=0\n",
    "for j in range(5):\n",
    "    ii=-1\n",
    "    for i in range(0,len_,round(len_/10)):\n",
    "        ii+=1\n",
    "        right=i+round(len_/10)\n",
    "        if right>=len_:\n",
    "            break\n",
    "        print(i,right)\n",
    "        df_train_user_doc=pd.read_pickle('../../data/wj/df_train_user_doc_0_1_'+str(i)+'_'+str(right)+'_64.pkl')\n",
    "        df_train_user_doc=pd.merge(df_train_user_doc,title_,how='left',on='docid')\n",
    "        \n",
    "        train, valid = train_test_split(df_train_user_doc, test_size=0.25, random_state=2021)\n",
    "#         train_loader，valid_loader\n",
    "        train_dataset = Data.TensorDataset(torch.LongTensor(train[sparse_features].values),\n",
    "                                           torch.FloatTensor(train[dense_features].values),\n",
    "                                           torch.FloatTensor(np.stack(train['title_'].values,axis=0)),\n",
    "                                           torch.FloatTensor(train['click'].values), )\n",
    "        train_loader = Data.DataLoader(dataset=train_dataset, batch_size=8192, shuffle=True)\n",
    "        valid_dataset = Data.TensorDataset(torch.LongTensor(valid[sparse_features].values),\n",
    "                                           torch.FloatTensor(valid[dense_features].values),\n",
    "                                           torch.FloatTensor(np.stack(valid['title_'].values,axis=0)),\n",
    "                                           torch.FloatTensor(valid['click'].values))\n",
    "        valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=8192, shuffle=False)\n",
    "#         train\n",
    "        print('train')\n",
    "        epoch_=1\n",
    "        best_auc=train_and_eval(model, train_loader, valid_loader, epoch_, optimizer, loss_fcn, scheduler, device,best_auc,j,ii)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/wj/deepfm_best_\"+str(0.7813)+\"_\"+str(4)+\"_\"+str(9)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "def predict(test_df, s_feat, den_feat, model, device):\n",
    "    test_dataset = Data.TensorDataset(torch.LongTensor(test_df[s_feat].values),\n",
    "                                       torch.FloatTensor(test_df[den_feat].values))\n",
    "    test_loader = Data.DataLoader(dataset=test_dataset, batch_size=4096, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for idx, x in tqdm(enumerate(test_loader)):\n",
    "            cate_fea, nume_fea = x[0], x[1]\n",
    "            cate_fea, nume_fea = cate_fea.to(device), nume_fea.to(device)\n",
    "            pred = model(cate_fea, nume_fea).reshape(-1).data.cpu().numpy().tolist()\n",
    "            test_preds.extend(pred)\n",
    "        id_list = list(range(0, len(test_preds)))\n",
    "        out_dict = {\"id\": id_list, \"pred\": test_preds}\n",
    "        out_df = pd.DataFrame(out_dict)\n",
    "        end=time.time()\n",
    "        out_df.to_csv('../../data/wj/hand_'+str(end)+'.csv', sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_user_doc=pd.read_pickle('../../data/wj/df_test_user_doc.pkl')\n",
    "model.eval()  # 把模型转为test模式\n",
    "predict(df_test_user_doc, sparse_features, dense_features, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_user_doc.to_pickle('../../data/wj/df_train_user_doc_0.1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))\n",
    "# model.load_state_dict(torch.load('../../data/wj/deepfm_best_0.7739082162724398_1639060823.4660978.pth'))\n",
    "# model.to(device)\n",
    "# epoch = 5\n",
    "# train_and_eval(model, train_loader, valid_loader, epoch, optimizer, loss_fcn, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_user_doc=pd.read_pickle('../../data/wj/df_test_user_doc.pkl')\n",
    "doc_feat = pd.read_pickle('../../data/wj/doc.pkl')\n",
    "user_feat = pd.read_pickle('../../data/wj/user.pkl')\n",
    "\n",
    "\n",
    "sparse_features = ['userid', 'docid', 'network', 'hour', 'device', 'os', 'province',\n",
    "                   'city', 'age', 'gender', 'category1st', 'category2nd',\n",
    "                   'pub_date', 'keyword0', 'keyword1', 'keyword2', 'keyword3', 'keyword4']\n",
    "\n",
    "dense_features = ['refresh', 'picnum',\n",
    "                  'userid_click_mean','userid_click_count' ,'userid_duration_mean' ,'userid_picnum_mean',\n",
    "                    'docid_click_mean','docid_click_count','docid_duration_mean','docid_picnum_mean',\n",
    "                    'category1st_click_mean','category1st_click_count','category1st_duration_mean','category1st_picnum_mean',\n",
    "                    'category2nd_click_mean','category2nd_click_count','category2nd_duration_mean','category2nd_picnum_mean',\n",
    "                    'keyword0_click_mean','keyword0_click_count','keyword0_duration_mean','keyword0_picnum_mean',\n",
    "                 'network_click_mean', 'network_click_count', 'network_duration_mean', \n",
    "                  'refresh_click_mean', 'refresh_click_count', 'refresh_duration_mean',\n",
    "                  'device_click_mean', 'device_click_count', 'device_duration_mean', \n",
    "                  'os_click_mean', 'os_click_count', 'os_duration_mean', \n",
    "                  'province_click_mean', 'province_click_count', 'province_duration_mean', \n",
    "                  'city_click_mean', 'city_click_count', 'city_duration_mean', \n",
    "                  'age_click_mean', 'age_click_count', 'age_duration_mean', \n",
    "                  'gender_click_mean', 'gender_click_count', 'gender_duration_mean'\n",
    "                 ]\n",
    "\n",
    "cate_fea_nuniqs = []\n",
    "cate_fea_nuniqs.append(user_feat['userid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(doc_feat['docid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(6)  # network\n",
    "cate_fea_nuniqs.append(13)  # hour\n",
    "cate_fea_nuniqs.append(user_feat['device'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['os'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['province'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['city'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['age'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['gender'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category1st'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category2nd'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['pub_date'].nunique())\n",
    "keyword_nunique = max(doc_feat['keyword0'].max(), doc_feat['keyword1'].max(), doc_feat['keyword2'].max()\n",
    "                      , doc_feat['keyword3'].max(), doc_feat['keyword4'].max()) + 1\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:00, 13.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))\n",
    "# model.load_state_dict(torch.load('../../data/wj/deepfm_best_0.7739082162724398_1639060823.4660978.pth'))\n",
    "\n",
    "device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model = model.to(device)\n",
    "model.eval()  # 把模型转为test模式\n",
    "predict(df_test_user_doc, sparse_features, dense_features, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
