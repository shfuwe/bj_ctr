{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import time, json, datetime\n",
    "from tqdm import tqdm\n",
    "import sys, getopt\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')\n",
    "nono=[]\n",
    "# nono=['userid_duration_mean','userid_click_mean','docid_click_mean','refresh','keyword0_click_mean','refresh_duration_mean','refresh_click_count']\n",
    "pp='3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class fn_cls(nn.Module):\n",
    "#     def __init__(self,device,nume_fea_size):\n",
    "#         super(fn_cls, self).__init__()\n",
    "#         self.model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "#         self.model.resize_token_embeddings(len(tokenizer))##############\n",
    "#         self.model.to(device)\n",
    "# #         self.dropout = nn.Dropout(0.5)\n",
    "#         self.l1 = nn.Linear(768, 1)\n",
    "#         self.l2 = nn.Linear(nume_fea_size, 1)\n",
    "#         self.l3 = nn.Linear(2, 1)\n",
    "#     def forward(self, nume_fea, input_ids, attention_mask):\n",
    "#         outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "# #         print(outputs[0])torch.Size([8, 100, 768])\n",
    "# #         print(outputs[1])torch.Size([8, 768])\n",
    "# #         print(outputs[0][:,0,:])torch.Size([8, 768])\n",
    "#         bert_cls = outputs[1]\n",
    "# #         x = self.dropout(x)\n",
    "#         x_bert = self.l1(bert_cls)\n",
    "    \n",
    "#         x_dense= self.l2(nume_fea)\n",
    "#         x=self.l3(torch.cat((x_bert,x_dense),1))\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "sigmoid=nn.Sigmoid()\n",
    "\n",
    "class fn_cls(nn.Module):\n",
    "    def __init__(self,device,nume_fea_size):\n",
    "        super(fn_cls, self).__init__()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(300, 1)\n",
    "        self.l2 = nn.Linear(nume_fea_size, 1)\n",
    "        self.l3 = nn.Linear(2, 1)\n",
    "    def forward(self, nume_fea, emb):\n",
    "        x_title = self.l1(emb)\n",
    "        x_dense= self.l2(nume_fea)\n",
    "        x=self.l3(torch.cat((x_title,x_dense),1))\n",
    "        \n",
    "        return sigmoid(x_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def train_and_eval(model, train_loader, valid_loader, epochs, optimizer, loss_fcn, scheduler, device,best_auc,j,ii):\n",
    "#     import time\n",
    "#     for _ in range(epochs):\n",
    "#         \"\"\"训练部分\"\"\"\n",
    "        \n",
    "#         model.to(device)\n",
    "#         model.train()\n",
    "        \n",
    "#         print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "#         write_log('Epoch: {}:{}'.format(j,ii))\n",
    "#         train_loss_sum = 0.0\n",
    "#         start_time = time.time()\n",
    "#         for idx, x in enumerate(train_loader):\n",
    "#             doc_id, nume_fea, label = x[0], x[1], x[2]\n",
    "#             doc_id=pd.DataFrame(doc_id,columns=['docid'])\n",
    "        \n",
    "#             doc_title=pd.merge(doc_id,title_,how='left',on='docid')\n",
    "#             input_ids=torch.LongTensor(np.stack(doc_title['input_ids'],axis=0))\n",
    "#             attention_mask=torch.LongTensor(np.stack(doc_title['attention_mask'],axis=0))\n",
    "            \n",
    "            \n",
    "#             nume_fea, input_ids, attention_mask, label = nume_fea.to(device), input_ids.to(device), attention_mask.to(device), label.float().to(device)\n",
    "# #             print(input_ids)\n",
    "#             pred = model(nume_fea, input_ids, attention_mask).view(-1)\n",
    "#             loss = loss_fcn(pred, label)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             train_loss_sum += loss.cpu().item()\n",
    "#             if (idx + 1) % 50 == 0 or (idx + 1) == len(train_loader):\n",
    "#                 write_log(\"Epoch {:04d} | Step {:04d} / {} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "#                     _ + 1, idx + 1, len(train_loader), train_loss_sum / (idx + 1), time.time() - start_time))\n",
    "#         scheduler.step()\n",
    "#         \"\"\"推断部分\"\"\"\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             valid_labels, valid_preds = [], []\n",
    "#             for idx, x in tqdm(enumerate(valid_loader)):\n",
    "#                 doc_id, nume_fea, label = x[0], x[1], x[2]\n",
    "#                 doc_id=pd.DataFrame(doc_id,columns=['docid'])\n",
    "                \n",
    "#                 doc_title=pd.merge(doc_id,title_,how='left',on='docid')\n",
    "#                 input_ids=torch.LongTensor(np.stack(doc_title['input_ids'],axis=0))\n",
    "#                 attention_mask=torch.LongTensor(np.stack(doc_title['attention_mask'],axis=0))\n",
    "                \n",
    "#                 nume_fea, input_ids, attention_mask = nume_fea.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "#                 pred = model(nume_fea, input_ids, attention_mask).reshape(-1).data.cpu().numpy().tolist()\n",
    "#                 valid_preds.extend(pred)\n",
    "#                 valid_labels.extend(label.cpu().numpy().tolist())\n",
    "#         cur_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "#         if cur_auc > best_auc:\n",
    "#             best_auc = cur_auc\n",
    "#             import time\n",
    "#             end=time.time()\n",
    "#             torch.save(model.state_dict(), \"../../data/wj/bert_best/\"+pp+\"/bert_best_\"+str(round(best_auc,4))+\"_\"+str(j)+\"_\"+str(ii)+\"_\"+str(time.strftime('%m_%d_%H_%M_%S'))+\".pth\")\n",
    "#         write_log('Current AUC: %.6f, Best AUC: %.6f\\n' % (cur_auc, best_auc))\n",
    "#     return best_auc\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, epochs, optimizer, loss_fcn, scheduler, device,best_auc,j,ii):\n",
    "    import time\n",
    "    for _ in range(epochs):\n",
    "        \"\"\"训练部分\"\"\"\n",
    "        \n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        write_log('Epoch: {}:{}'.format(j,ii))\n",
    "        train_loss_sum = 0.0\n",
    "        start_time = time.time()\n",
    "        for idx, x in enumerate(train_loader):\n",
    "            doc_id, nume_fea, label = x[0], x[1], x[2]\n",
    "            doc_id=pd.DataFrame(doc_id,columns=['docid'])\n",
    "            doc_title=pd.merge(doc_id,title_,how='left',on='docid')\n",
    "            title_0=torch.FloatTensor(np.stack(doc_title['title_'],axis=0))\n",
    "            \n",
    "            nume_fea, title_0, label = nume_fea.to(device), title_0.to(device), label.float().to(device)\n",
    "#             print(input_ids)\n",
    "            pred = model(nume_fea, title_0).view(-1)\n",
    "            loss = loss_fcn(pred, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.cpu().item()\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == len(train_loader):\n",
    "                write_log(\"Epoch {:04d} | Step {:04d} / {} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                    _ + 1, idx + 1, len(train_loader), train_loss_sum / (idx + 1), time.time() - start_time))\n",
    "        scheduler.step()\n",
    "        \"\"\"推断部分\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_labels, valid_preds = [], []\n",
    "            for idx, x in tqdm(enumerate(valid_loader)):\n",
    "                doc_id, nume_fea, label = x[0], x[1], x[2]\n",
    "                doc_id=pd.DataFrame(doc_id,columns=['docid'])\n",
    "                doc_title=pd.merge(doc_id,title_,how='left',on='docid')\n",
    "                title_0=torch.FloatTensor(np.stack(doc_title['title_'],axis=0))\n",
    "                \n",
    "                nume_fea, title_0 = nume_fea.to(device), title_0.to(device)\n",
    "                pred = model(nume_fea, title_0).reshape(-1).data.cpu().numpy().tolist()\n",
    "                valid_preds.extend(pred)\n",
    "                valid_labels.extend(label.cpu().numpy().tolist())\n",
    "        cur_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "        if cur_auc > best_auc:\n",
    "            best_auc = cur_auc\n",
    "            import time\n",
    "            end=time.time()\n",
    "            torch.save(model.state_dict(), \"../../data/wj/bert_best/\"+pp+\"/bert_best_\"+str(round(best_auc,4))+\"_\"+str(j)+\"_\"+str(ii)+\"_\"+str(time.strftime('%m_%d_%H_%M_%S'))+\".pth\")\n",
    "        write_log('Current AUC: %.6f, Best AUC: %.6f\\n' % (cur_auc, best_auc))\n",
    "    return best_auc\n",
    "\n",
    "\n",
    "import pytz\n",
    "import datetime\n",
    "tz = pytz.timezone('Asia/Shanghai')\n",
    "\n",
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "def write_log(w):\n",
    "    file_name = '../../data/wj/bert_best/'+pp+'/' + datetime.date.today().strftime('%m%d') + \"_{}.log\".format(\"bert\")\n",
    "    t0 = datetime.datetime.now(tz).strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(info + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 46\n"
     ]
    }
   ],
   "source": [
    "doc_feat = pd.read_pickle('../../data/wj/doc.pkl')\n",
    "user_feat = pd.read_pickle('../../data/wj/user.pkl')\n",
    "\n",
    "sparse_features = ['userid', 'docid', 'network', 'hour', 'device', 'os', 'province',\n",
    "                   'city', 'age', 'gender', 'category1st', 'category2nd',\n",
    "                   'pub_date', 'keyword0', 'keyword1', 'keyword2', 'keyword3', 'keyword4']\n",
    "\n",
    "dense_features0 = ['refresh', 'picnum',\n",
    "                  'userid_click_mean','userid_click_count' ,'userid_duration_mean' ,'userid_picnum_mean',\n",
    "                   'docid_click_mean','docid_click_count','docid_duration_mean','docid_picnum_mean',\n",
    "                    'category1st_click_mean','category1st_click_count','category1st_duration_mean','category1st_picnum_mean',\n",
    "                    'category2nd_click_mean','category2nd_click_count','category2nd_duration_mean','category2nd_picnum_mean',\n",
    "                    'keyword0_click_mean','keyword0_click_count','keyword0_duration_mean','keyword0_picnum_mean',\n",
    "                 'network_click_mean', 'network_click_count', 'network_duration_mean', \n",
    "                  'refresh_click_mean', 'refresh_click_count', 'refresh_duration_mean',\n",
    "                  'device_click_mean', 'device_click_count', 'device_duration_mean', \n",
    "                  'os_click_mean', 'os_click_count', 'os_duration_mean', \n",
    "                  'province_click_mean', 'province_click_count', 'province_duration_mean', \n",
    "                  'city_click_mean', 'city_click_count', 'city_duration_mean', \n",
    "                  'age_click_mean', 'age_click_count', 'age_duration_mean', \n",
    "                  'gender_click_mean', 'gender_click_count', 'gender_duration_mean'\n",
    "                 ]\n",
    "\n",
    "dense_features=[]\n",
    "for i in dense_features0:\n",
    "    if i not in nono:\n",
    "        dense_features.append(i)\n",
    "print(len(dense_features0),len(dense_features))\n",
    "\n",
    "cate_fea_nuniqs = []\n",
    "cate_fea_nuniqs.append(user_feat['userid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(doc_feat['docid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(6)  # network\n",
    "cate_fea_nuniqs.append(13)  # hour\n",
    "cate_fea_nuniqs.append(user_feat['device'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['os'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['province'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['city'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['age'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['gender'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category1st'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category2nd'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['pub_date'].nunique())\n",
    "keyword_nunique = max(doc_feat['keyword0'].max(), doc_feat['keyword1'].max(), doc_feat['keyword2'].max()\n",
    "                      , doc_feat['keyword3'].max(), doc_feat['keyword4'].max()) + 1\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "title_=pd.read_pickle('../../data/wj/title_embedding300.pkl')\n",
    "del title_['title']\n",
    "# title_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# title_=pd.read_csv('../../data/wj/doc_title.csv')\n",
    "# text=title_['title'].tolist()\n",
    "# for i in range(len(text)):\n",
    "#     if not isinstance(text[i], str):\n",
    "#         print(text[i])\n",
    "#         text[i]=''\n",
    "\n",
    "\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# # added_token=['##char##']\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\",additional_special_tokens=added_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "# text2id=tokenizer(text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "# nput_ids=text2id[\"input_ids\"].tolist()\n",
    "# attention_mask=text2id[\"attention_mask\"].tolist()\n",
    "# title_['input_ids']=nput_ids\n",
    "# title_['attention_mask']=attention_mask\n",
    "\n",
    "# title_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = fn_cls(device,nume_fea_size=len(dense_features))\n",
    "\n",
    "loss_fcn = nn.MSELoss()\n",
    "loss_fcn = loss_fcn.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# len_=81671133\n",
    "# best_auc=0\n",
    "# for j in range(5):\n",
    "#     ii=-1\n",
    "#     for i in range(0,len_,round(len_/10)):\n",
    "#         ii+=1\n",
    "#         right=i+round(len_/10)\n",
    "#         if right>=len_:\n",
    "#             break\n",
    "#         print(i,right)\n",
    "#         df_train_user_doc=pd.read_pickle('../../data/wj/df_train_user_doc_0_1_'+str(i)+'_'+str(right)+'_64.pkl')\n",
    "# #         df_train_user_doc=pd.merge(df_train_user_doc,title_,how='left',on='docid')\n",
    "        \n",
    "#         train, valid = train_test_split(df_train_user_doc, test_size=0.25, random_state=2021)\n",
    "# #         train_loader，valid_loader\n",
    "#         train_dataset = Data.TensorDataset(torch.LongTensor(np.stack(train['docid'].values,axis=0)),\n",
    "#                                            torch.FloatTensor(train[dense_features].values),\n",
    "#                                            torch.FloatTensor(train['click'].values), )\n",
    "#         train_loader = Data.DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "        \n",
    "#         valid_dataset = Data.TensorDataset(torch.LongTensor(np.stack(valid['docid'].values,axis=0)),\n",
    "#                                            torch.FloatTensor(valid[dense_features].values),\n",
    "#                                            torch.FloatTensor(valid['click'].values))\n",
    "#         valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=16, shuffle=False)\n",
    "# #         train\n",
    "#         print('train')\n",
    "#         epoch_=1\n",
    "#         best_auc=train_and_eval(model, train_loader, valid_loader, epoch_, optimizer, loss_fcn, scheduler, device,best_auc,j,ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8167113\n",
      "train\n",
      "Current lr : 0.005\n",
      "00:26:17 : Epoch: 0:0\n",
      "00:26:34 : Epoch 0001 | Step 0050 / 748 | Loss 0.1350 | Time 16.9269\n",
      "00:26:49 : Epoch 0001 | Step 0100 / 748 | Loss 0.1298 | Time 32.3504\n",
      "00:27:05 : Epoch 0001 | Step 0150 / 748 | Loss 0.1278 | Time 47.8488\n",
      "00:27:21 : Epoch 0001 | Step 0200 / 748 | Loss 0.1270 | Time 63.6701\n",
      "00:27:37 : Epoch 0001 | Step 0250 / 748 | Loss 0.1264 | Time 79.8634\n",
      "00:27:52 : Epoch 0001 | Step 0300 / 748 | Loss 0.1260 | Time 95.3600\n",
      "00:28:08 : Epoch 0001 | Step 0350 / 748 | Loss 0.1257 | Time 110.6146\n",
      "00:28:23 : Epoch 0001 | Step 0400 / 748 | Loss 0.1255 | Time 125.5845\n",
      "00:28:38 : Epoch 0001 | Step 0450 / 748 | Loss 0.1253 | Time 141.0486\n",
      "00:28:54 : Epoch 0001 | Step 0500 / 748 | Loss 0.1251 | Time 156.6900\n",
      "00:29:09 : Epoch 0001 | Step 0550 / 748 | Loss 0.1250 | Time 172.0465\n",
      "00:29:25 : Epoch 0001 | Step 0600 / 748 | Loss 0.1250 | Time 187.7130\n",
      "00:29:40 : Epoch 0001 | Step 0650 / 748 | Loss 0.1249 | Time 202.8594\n",
      "00:29:56 : Epoch 0001 | Step 0700 / 748 | Loss 0.1248 | Time 218.6550\n",
      "00:30:10 : Epoch 0001 | Step 0748 / 748 | Loss 0.1247 | Time 233.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:13,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:31:25 : Current AUC: 0.533739, Best AUC: 0.533739\n",
      "\n",
      "8167113 16334226\n",
      "train\n",
      "Current lr : 0.004\n",
      "00:31:55 : Epoch: 0:1\n",
      "00:32:12 : Epoch 0001 | Step 0050 / 748 | Loss 0.1228 | Time 16.4495\n",
      "00:32:27 : Epoch 0001 | Step 0100 / 748 | Loss 0.1234 | Time 31.6977\n",
      "00:32:42 : Epoch 0001 | Step 0150 / 748 | Loss 0.1236 | Time 47.1438\n",
      "00:32:58 : Epoch 0001 | Step 0200 / 748 | Loss 0.1236 | Time 62.5096\n",
      "00:33:13 : Epoch 0001 | Step 0250 / 748 | Loss 0.1235 | Time 78.1752\n",
      "00:33:29 : Epoch 0001 | Step 0300 / 748 | Loss 0.1235 | Time 93.6950\n",
      "00:33:46 : Epoch 0001 | Step 0350 / 748 | Loss 0.1236 | Time 110.4730\n",
      "00:34:03 : Epoch 0001 | Step 0400 / 748 | Loss 0.1237 | Time 127.4348\n",
      "00:34:20 : Epoch 0001 | Step 0450 / 748 | Loss 0.1236 | Time 144.3466\n",
      "00:34:36 : Epoch 0001 | Step 0500 / 748 | Loss 0.1237 | Time 160.5123\n",
      "00:34:51 : Epoch 0001 | Step 0550 / 748 | Loss 0.1237 | Time 175.8768\n",
      "00:35:06 : Epoch 0001 | Step 0600 / 748 | Loss 0.1237 | Time 191.1427\n",
      "00:35:22 : Epoch 0001 | Step 0650 / 748 | Loss 0.1237 | Time 207.1288\n",
      "00:35:38 : Epoch 0001 | Step 0700 / 748 | Loss 0.1236 | Time 223.0274\n",
      "00:35:53 : Epoch 0001 | Step 0748 / 748 | Loss 0.1236 | Time 237.7963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:15,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:37:09 : Current AUC: 0.536324, Best AUC: 0.536324\n",
      "\n",
      "16334226 24501339\n",
      "train\n",
      "Current lr : 0.0032\n",
      "00:37:40 : Epoch: 0:2\n",
      "00:37:56 : Epoch 0001 | Step 0050 / 748 | Loss 0.1236 | Time 16.1882\n",
      "00:38:12 : Epoch 0001 | Step 0100 / 748 | Loss 0.1239 | Time 32.2491\n",
      "00:38:28 : Epoch 0001 | Step 0150 / 748 | Loss 0.1237 | Time 48.6090\n",
      "00:38:44 : Epoch 0001 | Step 0200 / 748 | Loss 0.1239 | Time 64.0721\n",
      "00:38:59 : Epoch 0001 | Step 0250 / 748 | Loss 0.1238 | Time 79.6333\n",
      "00:39:17 : Epoch 0001 | Step 0300 / 748 | Loss 0.1238 | Time 96.8439\n",
      "00:39:33 : Epoch 0001 | Step 0350 / 748 | Loss 0.1239 | Time 113.2427\n",
      "00:39:49 : Epoch 0001 | Step 0400 / 748 | Loss 0.1239 | Time 129.6099\n",
      "00:40:06 : Epoch 0001 | Step 0450 / 748 | Loss 0.1238 | Time 145.7006\n",
      "00:40:21 : Epoch 0001 | Step 0500 / 748 | Loss 0.1238 | Time 161.2362\n",
      "00:40:37 : Epoch 0001 | Step 0550 / 748 | Loss 0.1237 | Time 176.9408\n",
      "00:40:53 : Epoch 0001 | Step 0600 / 748 | Loss 0.1237 | Time 193.0584\n",
      "00:41:08 : Epoch 0001 | Step 0650 / 748 | Loss 0.1237 | Time 208.3520\n",
      "00:41:24 : Epoch 0001 | Step 0700 / 748 | Loss 0.1237 | Time 223.8945\n",
      "00:41:39 : Epoch 0001 | Step 0748 / 748 | Loss 0.1237 | Time 238.8738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:18,  3.52it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "len_=81671133\n",
    "best_auc=0\n",
    "for j in range(5):\n",
    "    ii=-1\n",
    "    for i in range(0,len_,round(len_/10)):\n",
    "        ii+=1\n",
    "        right=i+round(len_/10)\n",
    "        if right>=len_:\n",
    "            break\n",
    "        print(i,right)\n",
    "        df_train_user_doc=pd.read_pickle('../../data/wj/df_train_user_doc_0_1_'+str(i)+'_'+str(right)+'_64.pkl')\n",
    "#         df_train_user_doc=pd.merge(df_train_user_doc,title_,how='left',on='docid')\n",
    "        \n",
    "        train, valid = train_test_split(df_train_user_doc, test_size=0.25, random_state=2021)\n",
    "#         train_loader，valid_loader\n",
    "        train_dataset = Data.TensorDataset(torch.LongTensor(np.stack(train['docid'].values,axis=0)),\n",
    "                                           torch.FloatTensor(train[dense_features].values),\n",
    "                                           torch.FloatTensor(train['click'].values), )\n",
    "        train_loader = Data.DataLoader(dataset=train_dataset, batch_size=8192, shuffle=True)\n",
    "        \n",
    "        valid_dataset = Data.TensorDataset(torch.LongTensor(np.stack(valid['docid'].values,axis=0)),\n",
    "                                           torch.FloatTensor(valid[dense_features].values),\n",
    "                                           torch.FloatTensor(valid['click'].values))\n",
    "        valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=8192, shuffle=False)\n",
    "#         train\n",
    "        print('train')\n",
    "        epoch_=1\n",
    "        best_auc=train_and_eval(model, train_loader, valid_loader, epoch_, optimizer, loss_fcn, scheduler, device,best_auc,j,ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_user_doc=pd.read_pickle('../../data/wj/df_test_user_doc_64_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "def predict(test_df, s_feat, den_feat, model, device,modeln):\n",
    "    test_dataset = Data.TensorDataset(torch.LongTensor(np.stack(test_df['docid'].values,axis=0)),\n",
    "                                           torch.FloatTensor(test_df[dense_features].values))\n",
    "    test_loader = Data.DataLoader(dataset=test_dataset, batch_size=4096, shuffle=False)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for idx, x in tqdm(enumerate(test_loader)):\n",
    "            doc_id, nume_fea = x[0], x[1]\n",
    "            doc_id=pd.DataFrame(doc_id,columns=['docid'])\n",
    "            doc_title=pd.merge(doc_id,title_,how='left',on='docid')\n",
    "            title_0=torch.FloatTensor(np.stack(doc_title['title_'],axis=0))\n",
    "\n",
    "            nume_fea, title_0 = nume_fea.to(device), title_0.to(device)\n",
    "            pred = model(nume_fea, title_0).reshape(-1).data.cpu().numpy().tolist()\n",
    "            \n",
    "            test_preds.extend(pred)\n",
    "        id_list = list(range(0, len(test_preds)))\n",
    "        out_dict = {\"id\": id_list, \"pred\": test_preds}\n",
    "        out_df = pd.DataFrame(out_dict)\n",
    "        out_df.to_csv('../../data/wj/bert_best/'+pp+'/bert_result_user_'+modeln+'.csv', sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:02,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "modeln='0.7598_0_8_12_21_13_24_11'\n",
    "model = fn_cls(device,nume_fea_size=len(dense_features))\n",
    "model.load_state_dict(torch.load('../../data/wj/bert_best/'+pp+'/bert_best_'+modeln+'.pth'))\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()  # 把模型转为test模式\n",
    "predict(df_test_user_doc, sparse_features, dense_features, model, device,modeln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
