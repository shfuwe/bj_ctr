{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import time, json, datetime\n",
    "from tqdm import tqdm\n",
    "import sys, getopt\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8,\n",
    "                 hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]):\n",
    "        \"\"\"\n",
    "        cate_fea_nuniqs: 类别特征的唯一值个数列表，也就是每个类别特征的vocab_size所组成的列表\n",
    "        nume_fea_size: 数值特征的个数，该模型会考虑到输入全为类别型，即没有数值特征的情况\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cate_fea_size = len(cate_fea_nuniqs)\n",
    "        self.nume_fea_size = nume_fea_size\n",
    "\n",
    "        \"\"\"FM部分\"\"\"\n",
    "        # 一阶\n",
    "        if self.nume_fea_size != 0:\n",
    "            self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1)  # 数值特征的一阶表示\n",
    "        self.fm_1st_order_sparse_emb = nn.ModuleList([\n",
    "            nn.Embedding(voc_size, 1) for voc_size in cate_fea_nuniqs])  # 类别特征的一阶表示\n",
    "        self.fm_1st_order_title = nn.Linear(128, 1)\n",
    "        \n",
    "        # 二阶\n",
    "        self.fm_2nd_order_sparse_emb = nn.ModuleList([\n",
    "            nn.Embedding(voc_size, emb_size) for voc_size in cate_fea_nuniqs])  # 类别特征的二阶表示\n",
    "\n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        self.all_dims = [self.cate_fea_size * emb_size] + hid_dims\n",
    "        self.dense_linear = nn.Linear(self.nume_fea_size, self.cate_fea_size * emb_size)  # 数值特征的维度变换到FM输出维度一致\n",
    "        self.title_linear = nn.Linear(128, self.cate_fea_size * emb_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # for DNN\n",
    "        for i in range(1, len(self.all_dims)):\n",
    "            setattr(self, 'linear_' + str(i), nn.Linear(self.all_dims[i - 1], self.all_dims[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(self.all_dims[i]))\n",
    "            setattr(self, 'activation_' + str(i), nn.ReLU())\n",
    "            setattr(self, 'dropout_' + str(i), nn.Dropout(dropout[i - 1]))\n",
    "        # for output\n",
    "        self.dnn_linear = nn.Linear(hid_dims[-1], num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X_sparse, X_dense, X_title):\n",
    "        \"\"\"\n",
    "        X_sparse: 类别型特征输入  [bs, cate_fea_size]\n",
    "        X_dense: 数值型特征输入（可能没有）  [bs, dense_fea_size]\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"FM 一阶部分\"\"\"\n",
    "        fm_1st_sparse_res = [emb(X_sparse[:, i].unsqueeze(1)).view(-1, 1)\n",
    "                             for i, emb in enumerate(self.fm_1st_order_sparse_emb)]\n",
    "        fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim=1)  # [bs, cate_fea_size]\n",
    "        fm_1st_sparse_res = torch.sum(fm_1st_sparse_res, 1, keepdim=True)  # [bs, 1]\n",
    "        \n",
    "        fm_1st_title_res=self.fm_1st_order_title(X_title)\n",
    "        \n",
    "        if X_dense is not None:\n",
    "            fm_1st_dense_res = self.fm_1st_order_dense(X_dense)\n",
    "            fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res+fm_1st_title_res\n",
    "        else:\n",
    "            fm_1st_part = fm_1st_sparse_res+fm_1st_title_res  # [bs, 1]\n",
    "        \"\"\"FM 一阶部分\"\"\"\n",
    "        \n",
    "        \"\"\"FM 二阶部分\"\"\"\n",
    "        fm_2nd_order_res = [emb(X_sparse[:, i].unsqueeze(1)) for i, emb in enumerate(self.fm_2nd_order_sparse_emb)]\n",
    "        fm_2nd_concat_1d = torch.cat(fm_2nd_order_res, dim=1)  # [bs, n, emb_size]  n为类别型特征个数(cate_fea_size)\n",
    "\n",
    "        # 先求和再平方\n",
    "        sum_embed = torch.sum(fm_2nd_concat_1d, 1)  # [bs, emb_size]\n",
    "        square_sum_embed = sum_embed * sum_embed  # [bs, emb_size]\n",
    "        # 先平方再求和\n",
    "        square_embed = fm_2nd_concat_1d * fm_2nd_concat_1d  # [bs, n, emb_size]\n",
    "        sum_square_embed = torch.sum(square_embed, 1)  # [bs, emb_size]\n",
    "        # 相减除以2\n",
    "        sub = square_sum_embed - sum_square_embed\n",
    "        sub = sub * 0.5  # [bs, emb_size]\n",
    "\n",
    "        fm_2nd_part = torch.sum(sub, 1, keepdim=True)  # [bs, 1]\n",
    "        \"\"\"FM 二阶部分\"\"\"\n",
    "        \n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        dnn_out = torch.flatten(fm_2nd_concat_1d, 1)  # [bs, n * emb_size]\n",
    "        if X_title is not None:\n",
    "            title_out=self.relu(self.title_linear(X_title))\n",
    "            dnn_out= dnn_out+title_out\n",
    "        \n",
    "        if X_dense is not None:\n",
    "            dense_out = self.relu(self.dense_linear(X_dense))  # [bs, n * emb_size]\n",
    "            dnn_out = dnn_out + dense_out  # [bs, n * emb_size]\n",
    "\n",
    "        for i in range(1, len(self.all_dims)):\n",
    "            dnn_out = getattr(self, 'linear_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'batchNorm_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'activation_' + str(i))(dnn_out)\n",
    "            dnn_out = getattr(self, 'dropout_' + str(i))(dnn_out)\n",
    "\n",
    "        dnn_out = self.dnn_linear(dnn_out)  # [bs, 1]\n",
    "        \"\"\"DNN部分\"\"\"\n",
    "        \n",
    "        out = fm_1st_part + fm_2nd_part + dnn_out  # [bs, 1]\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_loader, valid_loader, epochs, optimizer, loss_fcn, scheduler, device,ii):\n",
    "    import time\n",
    "    best_auc=0\n",
    "    for _ in range(epochs):\n",
    "        \"\"\"训练部分\"\"\"\n",
    "        model.train()\n",
    "        print(\"Current lr : {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "        write_log('Epoch: {}:{}'.format(ii,_))\n",
    "        train_loss_sum = 0.0\n",
    "        start_time = time.time()\n",
    "        for idx, x in enumerate(train_loader):\n",
    "            cate_fea, nume_fea, title_fea, label = x[0], x[1], x[2] ,x[3]\n",
    "            cate_fea, nume_fea,title_fea, label = cate_fea.to(device), nume_fea.to(device),title_fea.to(device), label.float().to(device)\n",
    "            pred = model(cate_fea, nume_fea,title_fea).view(-1)\n",
    "            loss = loss_fcn(pred, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.cpu().item()\n",
    "            if (idx + 1) % 50 == 0 or (idx + 1) == len(train_loader):\n",
    "                write_log(\"Epoch {:04d} | Step {:04d} / {} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                    _ + 1, idx + 1, len(train_loader), train_loss_sum / (idx + 1), time.time() - start_time))\n",
    "        scheduler.step()\n",
    "        \"\"\"推断部分\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_labels, valid_preds = [], []\n",
    "            for idx, x in tqdm(enumerate(valid_loader)):\n",
    "                cate_fea, nume_fea,title_fea, label = x[0], x[1], x[2],x[3]\n",
    "                cate_fea, nume_fea,title_fea = cate_fea.to(device), nume_fea.to(device),title_fea.to(device)\n",
    "                pred = model(cate_fea, nume_fea,title_fea).reshape(-1).data.cpu().numpy().tolist()\n",
    "                valid_preds.extend(pred)\n",
    "                valid_labels.extend(label.cpu().numpy().tolist())\n",
    "        cur_auc = roc_auc_score(valid_labels, valid_preds)\n",
    "        if cur_auc > best_auc:\n",
    "            best_auc = cur_auc\n",
    "            import time\n",
    "            end=time.time()\n",
    "            torch.save(model.state_dict(), \"../../data/wj/deepfm_best/fen10 3.0/deepfm_best10fen_title_\"+str(round(best_auc,4))+\"_\"+str(ii)+\"_\"+str(_)+\"_\"+str(time.strftime('%m_%d_%H_%M_%S'))+\".pth\")\n",
    "        write_log('Current AUC: %.6f, Best AUC: %.6f\\n' % (cur_auc, best_auc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定义日志（data文件夹下，同级目录新建一个data文件夹）\n",
    "def write_log(w):\n",
    "    file_name = '../../data/wj/' + datetime.date.today().strftime('%m%d') + \"_{}.log\".format(\"deepfm_10fen\")\n",
    "    t0 = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "    info = \"{} : {}\".format(t0, w)\n",
    "    print(info)\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(info + '\\n')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_feat = pd.read_pickle('../../data/wj/doc.pkl')\n",
    "user_feat = pd.read_pickle('../../data/wj/user.pkl')\n",
    "df_test_user_doc=pd.read_pickle('../../data/wj/df_test_user_doc_64.pkl')\n",
    "\n",
    "sparse_features = ['userid', 'docid', 'network', 'hour', 'device', 'os', 'province',\n",
    "                   'city', 'age', 'gender', 'category1st', 'category2nd',\n",
    "                   'pub_date', 'keyword0', 'keyword1', 'keyword2', 'keyword3', 'keyword4']\n",
    "\n",
    "dense_features = ['refresh', 'picnum',\n",
    "                  'userid_click_mean','userid_click_count' ,'userid_duration_mean' ,'userid_picnum_mean',\n",
    "                    'docid_click_mean','docid_click_count','docid_duration_mean','docid_picnum_mean',\n",
    "                    'category1st_click_mean','category1st_click_count','category1st_duration_mean','category1st_picnum_mean',\n",
    "                    'category2nd_click_mean','category2nd_click_count','category2nd_duration_mean','category2nd_picnum_mean',\n",
    "                    'keyword0_click_mean','keyword0_click_count','keyword0_duration_mean','keyword0_picnum_mean',\n",
    "                 'network_click_mean', 'network_click_count', 'network_duration_mean', \n",
    "                  'refresh_click_mean', 'refresh_click_count', 'refresh_duration_mean',\n",
    "                  'device_click_mean', 'device_click_count', 'device_duration_mean', \n",
    "                  'os_click_mean', 'os_click_count', 'os_duration_mean', \n",
    "                  'province_click_mean', 'province_click_count', 'province_duration_mean', \n",
    "                  'city_click_mean', 'city_click_count', 'city_duration_mean', \n",
    "                  'age_click_mean', 'age_click_count', 'age_duration_mean', \n",
    "                  'gender_click_mean', 'gender_click_count', 'gender_duration_mean'\n",
    "                 ]\n",
    "\n",
    "cate_fea_nuniqs = []\n",
    "cate_fea_nuniqs.append(user_feat['userid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(doc_feat['docid'].nunique() + 1)\n",
    "cate_fea_nuniqs.append(6)  # network\n",
    "cate_fea_nuniqs.append(13)  # hour\n",
    "cate_fea_nuniqs.append(user_feat['device'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['os'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['province'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['city'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['age'].nunique())\n",
    "cate_fea_nuniqs.append(user_feat['gender'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category1st'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['category2nd'].nunique())\n",
    "cate_fea_nuniqs.append(doc_feat['pub_date'].nunique())\n",
    "keyword_nunique = max(doc_feat['keyword0'].max(), doc_feat['keyword1'].max(), doc_feat['keyword2'].max()\n",
    "                      , doc_feat['keyword3'].max(), doc_feat['keyword4'].max()) + 1\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "cate_fea_nuniqs.append(keyword_nunique)\n",
    "\n",
    "device = torch.device('cuda:2') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_fcn = nn.BCELoss()\n",
    "loss_fcn = loss_fcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "title_=pd.read_pickle('../../data/wj/title_embedding.pkl')\n",
    "del title_['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8167113\n",
      "train\n",
      "Current lr : 0.005\n",
      "09:27:01 : Epoch: 0:0\n",
      "09:27:22 : Epoch 0001 | Step 0050 / 748 | Loss 11.9431 | Time 21.1444\n",
      "09:27:41 : Epoch 0001 | Step 0100 / 748 | Loss 8.5201 | Time 39.7328\n",
      "09:27:59 : Epoch 0001 | Step 0150 / 748 | Loss 6.4106 | Time 57.7807\n",
      "09:28:17 : Epoch 0001 | Step 0200 / 748 | Loss 5.0532 | Time 76.4202\n",
      "09:28:36 : Epoch 0001 | Step 0250 / 748 | Loss 4.1820 | Time 94.5529\n",
      "09:28:53 : Epoch 0001 | Step 0300 / 748 | Loss 3.5776 | Time 111.9283\n",
      "09:29:10 : Epoch 0001 | Step 0350 / 748 | Loss 3.1349 | Time 129.3362\n",
      "09:29:28 : Epoch 0001 | Step 0400 / 748 | Loss 2.7975 | Time 147.2616\n",
      "09:29:46 : Epoch 0001 | Step 0450 / 748 | Loss 2.5319 | Time 165.1027\n",
      "09:30:04 : Epoch 0001 | Step 0500 / 748 | Loss 2.3177 | Time 183.1542\n",
      "09:30:22 : Epoch 0001 | Step 0550 / 748 | Loss 2.1413 | Time 201.1811\n",
      "09:30:40 : Epoch 0001 | Step 0600 / 748 | Loss 1.9937 | Time 218.9442\n",
      "09:30:57 : Epoch 0001 | Step 0650 / 748 | Loss 1.8685 | Time 236.4289\n",
      "09:31:15 : Epoch 0001 | Step 0700 / 748 | Loss 1.7609 | Time 254.2880\n",
      "09:31:32 : Epoch 0001 | Step 0748 / 748 | Loss 1.6709 | Time 271.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:14,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:32:49 : Current AUC: 0.760207, Best AUC: 0.760207\n",
      "\n",
      "Current lr : 0.004\n",
      "09:32:49 : Epoch: 0:1\n",
      "09:33:11 : Epoch 0002 | Step 0050 / 748 | Loss 0.3561 | Time 21.3875\n",
      "09:33:31 : Epoch 0002 | Step 0100 / 748 | Loss 0.3555 | Time 41.6168\n",
      "09:33:50 : Epoch 0002 | Step 0150 / 748 | Loss 0.3555 | Time 61.0931\n",
      "09:34:10 : Epoch 0002 | Step 0200 / 748 | Loss 0.3553 | Time 80.4266\n",
      "09:34:29 : Epoch 0002 | Step 0250 / 748 | Loss 0.3548 | Time 100.2456\n",
      "09:34:49 : Epoch 0002 | Step 0300 / 748 | Loss 0.3545 | Time 119.5732\n",
      "09:35:07 : Epoch 0002 | Step 0350 / 748 | Loss 0.3543 | Time 138.1268\n",
      "09:35:27 : Epoch 0002 | Step 0400 / 748 | Loss 0.3545 | Time 158.0096\n",
      "09:35:47 : Epoch 0002 | Step 0450 / 748 | Loss 0.3543 | Time 177.8406\n",
      "09:36:07 : Epoch 0002 | Step 0500 / 748 | Loss 0.3542 | Time 197.6384\n",
      "09:36:26 : Epoch 0002 | Step 0550 / 748 | Loss 0.3540 | Time 217.0410\n",
      "09:36:45 : Epoch 0002 | Step 0600 / 748 | Loss 0.3540 | Time 235.7293\n",
      "09:37:04 : Epoch 0002 | Step 0650 / 748 | Loss 0.3539 | Time 254.8986\n",
      "09:37:23 : Epoch 0002 | Step 0700 / 748 | Loss 0.3538 | Time 273.6246\n",
      "09:37:41 : Epoch 0002 | Step 0748 / 748 | Loss 0.3537 | Time 291.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:13,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:38:56 : Current AUC: 0.770861, Best AUC: 0.770861\n",
      "\n",
      "Current lr : 0.0032\n",
      "09:38:56 : Epoch: 0:2\n",
      "09:39:17 : Epoch 0003 | Step 0050 / 748 | Loss 0.3513 | Time 21.4117\n",
      "09:39:37 : Epoch 0003 | Step 0100 / 748 | Loss 0.3507 | Time 40.6053\n",
      "09:39:56 : Epoch 0003 | Step 0150 / 748 | Loss 0.3518 | Time 59.9693\n",
      "09:40:16 : Epoch 0003 | Step 0200 / 748 | Loss 0.3511 | Time 79.8630\n",
      "09:40:37 : Epoch 0003 | Step 0250 / 748 | Loss 0.3506 | Time 100.9327\n",
      "09:40:57 : Epoch 0003 | Step 0300 / 748 | Loss 0.3508 | Time 120.6489\n",
      "09:41:17 : Epoch 0003 | Step 0350 / 748 | Loss 0.3509 | Time 141.0619\n",
      "09:41:38 : Epoch 0003 | Step 0400 / 748 | Loss 0.3510 | Time 161.6528\n",
      "09:41:57 : Epoch 0003 | Step 0450 / 748 | Loss 0.3510 | Time 181.3358\n",
      "09:42:17 : Epoch 0003 | Step 0500 / 748 | Loss 0.3511 | Time 200.7855\n",
      "09:42:36 : Epoch 0003 | Step 0550 / 748 | Loss 0.3512 | Time 219.7763\n",
      "09:42:55 : Epoch 0003 | Step 0600 / 748 | Loss 0.3511 | Time 238.8442\n",
      "09:43:15 : Epoch 0003 | Step 0650 / 748 | Loss 0.3510 | Time 259.4049\n",
      "09:43:35 : Epoch 0003 | Step 0700 / 748 | Loss 0.3509 | Time 279.0007\n",
      "09:43:54 : Epoch 0003 | Step 0748 / 748 | Loss 0.3510 | Time 297.6342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:14,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:45:09 : Current AUC: 0.769947, Best AUC: 0.770861\n",
      "\n",
      "Current lr : 0.00256\n",
      "09:45:09 : Epoch: 0:3\n",
      "09:45:30 : Epoch 0004 | Step 0050 / 748 | Loss 0.3516 | Time 20.5735\n",
      "09:45:49 : Epoch 0004 | Step 0100 / 748 | Loss 0.3511 | Time 39.8710\n",
      "09:46:08 : Epoch 0004 | Step 0150 / 748 | Loss 0.3506 | Time 58.6939\n",
      "09:46:28 : Epoch 0004 | Step 0200 / 748 | Loss 0.3503 | Time 79.0423\n",
      "09:46:49 : Epoch 0004 | Step 0250 / 748 | Loss 0.3504 | Time 99.2500\n",
      "09:47:09 : Epoch 0004 | Step 0300 / 748 | Loss 0.3506 | Time 119.3496\n",
      "09:47:29 : Epoch 0004 | Step 0350 / 748 | Loss 0.3505 | Time 139.3995\n",
      "09:47:48 : Epoch 0004 | Step 0400 / 748 | Loss 0.3502 | Time 159.0115\n",
      "09:48:08 : Epoch 0004 | Step 0450 / 748 | Loss 0.3503 | Time 178.7210\n",
      "09:48:27 : Epoch 0004 | Step 0500 / 748 | Loss 0.3504 | Time 197.3111\n",
      "09:48:47 : Epoch 0004 | Step 0550 / 748 | Loss 0.3504 | Time 217.6840\n",
      "09:49:06 : Epoch 0004 | Step 0600 / 748 | Loss 0.3502 | Time 236.8713\n",
      "09:49:26 : Epoch 0004 | Step 0650 / 748 | Loss 0.3502 | Time 256.5714\n",
      "09:49:46 : Epoch 0004 | Step 0700 / 748 | Loss 0.3503 | Time 276.2715\n",
      "09:50:04 : Epoch 0004 | Step 0748 / 748 | Loss 0.3503 | Time 294.7352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:14,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:51:21 : Current AUC: 0.773461, Best AUC: 0.773461\n",
      "\n",
      "Current lr : 0.0020480000000000003\n",
      "09:51:21 : Epoch: 0:4\n",
      "09:51:43 : Epoch 0005 | Step 0050 / 748 | Loss 0.3491 | Time 22.4154\n",
      "09:52:03 : Epoch 0005 | Step 0100 / 748 | Loss 0.3495 | Time 42.3507\n",
      "09:52:23 : Epoch 0005 | Step 0150 / 748 | Loss 0.3498 | Time 61.9535\n",
      "09:52:42 : Epoch 0005 | Step 0200 / 748 | Loss 0.3499 | Time 81.2450\n",
      "09:53:01 : Epoch 0005 | Step 0250 / 748 | Loss 0.3497 | Time 100.2586\n",
      "09:53:20 : Epoch 0005 | Step 0300 / 748 | Loss 0.3499 | Time 119.4949\n",
      "09:53:39 : Epoch 0005 | Step 0350 / 748 | Loss 0.3499 | Time 138.8378\n",
      "09:53:58 : Epoch 0005 | Step 0400 / 748 | Loss 0.3499 | Time 157.7972\n",
      "09:54:19 : Epoch 0005 | Step 0450 / 748 | Loss 0.3499 | Time 177.9432\n",
      "09:54:39 : Epoch 0005 | Step 0500 / 748 | Loss 0.3499 | Time 198.6067\n",
      "09:54:59 : Epoch 0005 | Step 0550 / 748 | Loss 0.3499 | Time 218.8064\n",
      "09:55:21 : Epoch 0005 | Step 0600 / 748 | Loss 0.3499 | Time 240.0711\n",
      "09:55:40 : Epoch 0005 | Step 0650 / 748 | Loss 0.3499 | Time 259.7705\n",
      "09:56:01 : Epoch 0005 | Step 0700 / 748 | Loss 0.3500 | Time 280.1489\n",
      "09:56:20 : Epoch 0005 | Step 0748 / 748 | Loss 0.3500 | Time 299.3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:15,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:37 : Current AUC: 0.773568, Best AUC: 0.773568\n",
      "\n",
      "Current lr : 0.0016384000000000004\n",
      "09:57:37 : Epoch: 0:5\n",
      "09:57:58 : Epoch 0006 | Step 0050 / 748 | Loss 0.3490 | Time 21.3753\n",
      "09:58:18 : Epoch 0006 | Step 0100 / 748 | Loss 0.3499 | Time 41.8553\n",
      "09:58:38 : Epoch 0006 | Step 0150 / 748 | Loss 0.3499 | Time 61.5889\n",
      "09:58:59 : Epoch 0006 | Step 0200 / 748 | Loss 0.3495 | Time 81.9445\n",
      "09:59:18 : Epoch 0006 | Step 0250 / 748 | Loss 0.3501 | Time 101.2094\n",
      "09:59:37 : Epoch 0006 | Step 0300 / 748 | Loss 0.3498 | Time 120.4679\n",
      "09:59:57 : Epoch 0006 | Step 0350 / 748 | Loss 0.3495 | Time 140.3274\n",
      "10:00:16 : Epoch 0006 | Step 0400 / 748 | Loss 0.3496 | Time 159.4522\n",
      "10:00:36 : Epoch 0006 | Step 0450 / 748 | Loss 0.3497 | Time 178.9731\n",
      "10:00:55 : Epoch 0006 | Step 0500 / 748 | Loss 0.3497 | Time 198.6236\n",
      "10:01:14 : Epoch 0006 | Step 0550 / 748 | Loss 0.3497 | Time 217.5238\n",
      "10:01:34 : Epoch 0006 | Step 0600 / 748 | Loss 0.3496 | Time 237.0188\n",
      "10:01:53 : Epoch 0006 | Step 0650 / 748 | Loss 0.3496 | Time 256.2072\n",
      "10:02:12 : Epoch 0006 | Step 0700 / 748 | Loss 0.3497 | Time 274.9687\n",
      "10:02:29 : Epoch 0006 | Step 0748 / 748 | Loss 0.3496 | Time 292.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:14,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:03:46 : Current AUC: 0.774880, Best AUC: 0.774880\n",
      "\n",
      "Current lr : 0.0013107200000000005\n",
      "10:03:46 : Epoch: 0:6\n",
      "10:04:09 : Epoch 0007 | Step 0050 / 748 | Loss 0.3498 | Time 22.8212\n",
      "10:04:29 : Epoch 0007 | Step 0100 / 748 | Loss 0.3492 | Time 42.7776\n",
      "10:04:49 : Epoch 0007 | Step 0150 / 748 | Loss 0.3489 | Time 63.3153\n",
      "10:05:10 : Epoch 0007 | Step 0200 / 748 | Loss 0.3489 | Time 84.2006\n",
      "10:05:30 : Epoch 0007 | Step 0250 / 748 | Loss 0.3490 | Time 104.1476\n",
      "10:05:50 : Epoch 0007 | Step 0300 / 748 | Loss 0.3492 | Time 124.1910\n",
      "10:06:09 : Epoch 0007 | Step 0350 / 748 | Loss 0.3492 | Time 143.2147\n",
      "10:06:28 : Epoch 0007 | Step 0400 / 748 | Loss 0.3496 | Time 161.9162\n",
      "10:06:47 : Epoch 0007 | Step 0450 / 748 | Loss 0.3498 | Time 181.0578\n",
      "10:07:05 : Epoch 0007 | Step 0500 / 748 | Loss 0.3496 | Time 199.4022\n",
      "10:07:24 : Epoch 0007 | Step 0550 / 748 | Loss 0.3496 | Time 218.4269\n",
      "10:07:43 : Epoch 0007 | Step 0600 / 748 | Loss 0.3495 | Time 237.4699\n",
      "10:08:02 : Epoch 0007 | Step 0650 / 748 | Loss 0.3495 | Time 256.0816\n",
      "10:08:21 : Epoch 0007 | Step 0700 / 748 | Loss 0.3495 | Time 275.2378\n",
      "10:08:39 : Epoch 0007 | Step 0748 / 748 | Loss 0.3495 | Time 292.9453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:13,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:09:54 : Current AUC: 0.775880, Best AUC: 0.775880\n",
      "\n",
      "Current lr : 0.0010485760000000005\n",
      "10:09:54 : Epoch: 0:7\n",
      "10:10:14 : Epoch 0008 | Step 0050 / 748 | Loss 0.3479 | Time 19.9964\n",
      "10:10:34 : Epoch 0008 | Step 0100 / 748 | Loss 0.3485 | Time 39.4321\n",
      "10:10:53 : Epoch 0008 | Step 0150 / 748 | Loss 0.3487 | Time 58.3469\n",
      "10:11:12 : Epoch 0008 | Step 0200 / 748 | Loss 0.3488 | Time 77.2907\n",
      "10:11:31 : Epoch 0008 | Step 0250 / 748 | Loss 0.3490 | Time 96.3770\n",
      "10:11:49 : Epoch 0008 | Step 0300 / 748 | Loss 0.3488 | Time 114.9544\n",
      "10:12:09 : Epoch 0008 | Step 0350 / 748 | Loss 0.3488 | Time 134.8025\n",
      "10:12:29 : Epoch 0008 | Step 0400 / 748 | Loss 0.3490 | Time 155.0876\n",
      "10:12:50 : Epoch 0008 | Step 0450 / 748 | Loss 0.3492 | Time 175.3018\n",
      "10:13:10 : Epoch 0008 | Step 0500 / 748 | Loss 0.3491 | Time 195.6243\n",
      "10:13:31 : Epoch 0008 | Step 0550 / 748 | Loss 0.3492 | Time 216.3692\n",
      "10:13:51 : Epoch 0008 | Step 0600 / 748 | Loss 0.3492 | Time 236.7076\n",
      "10:14:12 : Epoch 0008 | Step 0650 / 748 | Loss 0.3491 | Time 257.2455\n",
      "10:14:32 : Epoch 0008 | Step 0700 / 748 | Loss 0.3492 | Time 277.2022\n",
      "10:14:51 : Epoch 0008 | Step 0748 / 748 | Loss 0.3493 | Time 297.1493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:16,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:16:09 : Current AUC: 0.774511, Best AUC: 0.775880\n",
      "\n",
      "Current lr : 0.0008388608000000005\n",
      "10:16:09 : Epoch: 0:8\n",
      "10:16:30 : Epoch 0009 | Step 0050 / 748 | Loss 0.3492 | Time 20.8936\n",
      "10:16:50 : Epoch 0009 | Step 0100 / 748 | Loss 0.3494 | Time 40.1976\n",
      "10:17:09 : Epoch 0009 | Step 0150 / 748 | Loss 0.3496 | Time 59.6217\n",
      "10:17:29 : Epoch 0009 | Step 0200 / 748 | Loss 0.3495 | Time 79.3643\n",
      "10:17:48 : Epoch 0009 | Step 0250 / 748 | Loss 0.3492 | Time 98.3841\n",
      "10:18:08 : Epoch 0009 | Step 0300 / 748 | Loss 0.3492 | Time 118.9618\n",
      "10:18:29 : Epoch 0009 | Step 0350 / 748 | Loss 0.3492 | Time 139.2868\n",
      "10:18:50 : Epoch 0009 | Step 0400 / 748 | Loss 0.3493 | Time 160.1894\n",
      "10:19:10 : Epoch 0009 | Step 0450 / 748 | Loss 0.3490 | Time 180.4326\n",
      "10:19:31 : Epoch 0009 | Step 0500 / 748 | Loss 0.3491 | Time 201.0962\n",
      "10:19:52 : Epoch 0009 | Step 0550 / 748 | Loss 0.3491 | Time 222.7795\n",
      "10:20:13 : Epoch 0009 | Step 0600 / 748 | Loss 0.3490 | Time 243.4988\n",
      "10:20:33 : Epoch 0009 | Step 0650 / 748 | Loss 0.3490 | Time 263.6338\n",
      "10:20:52 : Epoch 0009 | Step 0700 / 748 | Loss 0.3491 | Time 283.0110\n",
      "10:21:11 : Epoch 0009 | Step 0748 / 748 | Loss 0.3491 | Time 301.2223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:15,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:22:28 : Current AUC: 0.774244, Best AUC: 0.775880\n",
      "\n",
      "Current lr : 0.0006710886400000004\n",
      "10:22:28 : Epoch: 0:9\n",
      "10:22:48 : Epoch 0010 | Step 0050 / 748 | Loss 0.3466 | Time 20.4464\n",
      "10:23:08 : Epoch 0010 | Step 0100 / 748 | Loss 0.3479 | Time 39.7958\n",
      "10:23:27 : Epoch 0010 | Step 0150 / 748 | Loss 0.3478 | Time 59.4082\n",
      "10:23:47 : Epoch 0010 | Step 0200 / 748 | Loss 0.3482 | Time 78.7784\n",
      "10:24:06 : Epoch 0010 | Step 0250 / 748 | Loss 0.3486 | Time 98.3309\n",
      "10:24:26 : Epoch 0010 | Step 0300 / 748 | Loss 0.3486 | Time 117.8223\n",
      "10:24:45 : Epoch 0010 | Step 0350 / 748 | Loss 0.3486 | Time 137.3632\n",
      "10:25:05 : Epoch 0010 | Step 0400 / 748 | Loss 0.3485 | Time 156.9472\n",
      "10:25:25 : Epoch 0010 | Step 0450 / 748 | Loss 0.3487 | Time 176.5433\n",
      "10:25:45 : Epoch 0010 | Step 0500 / 748 | Loss 0.3487 | Time 196.5601\n",
      "10:26:04 : Epoch 0010 | Step 0550 / 748 | Loss 0.3487 | Time 216.1945\n",
      "10:26:24 : Epoch 0010 | Step 0600 / 748 | Loss 0.3488 | Time 236.1412\n",
      "10:26:44 : Epoch 0010 | Step 0650 / 748 | Loss 0.3488 | Time 256.4444\n",
      "10:27:05 : Epoch 0010 | Step 0700 / 748 | Loss 0.3488 | Time 277.1110\n",
      "10:27:24 : Epoch 0010 | Step 0748 / 748 | Loss 0.3488 | Time 295.7751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:17,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:28:42 : Current AUC: 0.775772, Best AUC: 0.775880\n",
      "\n",
      "8167113 16334226\n",
      "train\n",
      "Current lr : 0.005\n",
      "10:29:35 : Epoch: 1:0\n",
      "10:29:57 : Epoch 0001 | Step 0050 / 748 | Loss 11.2514 | Time 21.6037\n",
      "10:30:16 : Epoch 0001 | Step 0100 / 748 | Loss 7.9494 | Time 40.5666\n",
      "10:30:35 : Epoch 0001 | Step 0150 / 748 | Loss 5.8798 | Time 60.0191\n",
      "10:30:54 : Epoch 0001 | Step 0200 / 748 | Loss 4.6397 | Time 78.9971\n",
      "10:31:13 : Epoch 0001 | Step 0250 / 748 | Loss 3.8463 | Time 98.0520\n",
      "10:31:33 : Epoch 0001 | Step 0300 / 748 | Loss 3.2952 | Time 117.7266\n",
      "10:31:52 : Epoch 0001 | Step 0350 / 748 | Loss 2.8917 | Time 136.8876\n",
      "10:32:11 : Epoch 0001 | Step 0400 / 748 | Loss 2.5837 | Time 155.8119\n",
      "10:32:29 : Epoch 0001 | Step 0450 / 748 | Loss 2.3413 | Time 174.0677\n",
      "10:32:48 : Epoch 0001 | Step 0500 / 748 | Loss 2.1457 | Time 192.5338\n",
      "10:33:06 : Epoch 0001 | Step 0550 / 748 | Loss 1.9850 | Time 211.2458\n",
      "10:33:24 : Epoch 0001 | Step 0600 / 748 | Loss 1.8505 | Time 229.3189\n",
      "10:33:43 : Epoch 0001 | Step 0650 / 748 | Loss 1.7362 | Time 247.8672\n",
      "10:34:02 : Epoch 0001 | Step 0700 / 748 | Loss 1.6381 | Time 267.4008\n",
      "10:34:20 : Epoch 0001 | Step 0748 / 748 | Loss 1.5559 | Time 285.2803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [01:19,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:35:42 : Current AUC: 0.758112, Best AUC: 0.758112\n",
      "\n",
      "Current lr : 0.004\n",
      "10:35:42 : Epoch: 1:1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "len_=81671133\n",
    "ii=-1\n",
    "for i in range(0,len_,round(len_/10)):\n",
    "    ii+=1\n",
    "    right=i+round(len_/10)\n",
    "    if right>=len_:\n",
    "        break\n",
    "    print(i,right)\n",
    "    df_train_user_doc=pd.read_pickle('../../data/wj/df_train_user_doc_0_1_'+str(i)+'_'+str(right)+'_64.pkl')\n",
    "    df_train_user_doc=pd.merge(df_train_user_doc,title_,how='left',on='docid')\n",
    "    \n",
    "    train, valid = train_test_split(df_train_user_doc, test_size=0.25, random_state=2021)\n",
    "#         train_loader，valid_loader\n",
    "    train_dataset = Data.TensorDataset(torch.LongTensor(train[sparse_features].values),\n",
    "                                           torch.FloatTensor(train[dense_features].values),\n",
    "                                           torch.FloatTensor(np.stack(train['title_'].values,axis=0)),\n",
    "                                           torch.FloatTensor(train['click'].values), )\n",
    "    train_loader = Data.DataLoader(dataset=train_dataset, batch_size=8192, shuffle=True)\n",
    "    valid_dataset = Data.TensorDataset(torch.LongTensor(valid[sparse_features].values),\n",
    "                                           torch.FloatTensor(valid[dense_features].values),\n",
    "                                           torch.FloatTensor(np.stack(valid['title_'].values,axis=0)),\n",
    "                                           torch.FloatTensor(valid['click'].values))\n",
    "    valid_loader = Data.DataLoader(dataset=valid_dataset, batch_size=8192, shuffle=False)\n",
    "    \n",
    "#         train\n",
    "    print('train')\n",
    "    model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "    epoch_=10\n",
    "    train_and_eval(model, train_loader, valid_loader, epoch_, optimizer, loss_fcn, scheduler, device,ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../data/wj/deepfm_best_\"+str(0.7791)+\"_\"+str(1)+\"_\"+str(9)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_user_doc.to_pickle('../../data/wj/df_train_user_doc_0.1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepFM(cate_fea_nuniqs, nume_fea_size=len(dense_features))\n",
    "# model.load_state_dict(torch.load('../../data/wj/deepfm_best_0.7739082162724398_1639060823.4660978.pth'))\n",
    "# model.to(device)\n",
    "# epoch = 5\n",
    "# train_and_eval(model, train_loader, valid_loader, epoch, optimizer, loss_fcn, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sw0",
   "language": "python",
   "name": "sw0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
